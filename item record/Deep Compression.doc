在服务器上跑了跑剪枝的代码，时间快了5分之4吧

关于代码的理解 ：简单的讲，它改了pytorch源码，重写了pytorch的线性层nn.Linear的代码，加了mask 的属性，mask的属性初始值是1对应于权重weight
（张量中元素一一对应），在前向求值的时候mask与weight对应相乘，就是当mask的元素被置零的时候，对应的权重是无效的（为0），它不会影响传递的结果，mask的置零是靠设置阈值，
即当权重小于某一阈值时mask被置零。（置零的规则来自于韩松的之前一篇文章）

主代码流程：
先训练一个网络（正常训练），之后剪枝（判定权重<阈值，mask置零），此时剪枝意味着前向求值的时候mask为0的元素对应的权重weight元素为0（张量相乘），
再训练，训练的时候清除值为0的weight的梯度，也就是最后计算损失更新weight时候，被置零的权重也不再更新。
这样就在逻辑上形成了剪枝，并没有节省计算量，或者改变网络的结构。


mask置零



前向传导时候权重与mask张量相乘（权重置零）


阻止后向传播weight值为0的参数更新


即逻辑上形成剪枝
某博客：


调包能做的事情更多.......
代码源：
https://github.com/mightydeveloper/Deep-Compression-PyTorch
文献：
DEEP COMPRESSION: COMPRESSING DEEP NEURAL 
NETWORKS WITH PRUNING, TRAINED QUANTIZATION 
AND HUFFMAN CODING
Song han
